<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head>
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>David Khudaverdyan.</title><base target="_top">
 
<script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
<script type="text/javascript" src="../menu.js"></script>
<script type="text/javascript" src="../spoiler.js"></script>
</head>
<link rel="stylesheet" type="text/css" href="../main.css">
<body  text="#000000" bgcolor="#FFFFFF" topmargin="0" leftmargin="0" rightmargin="0" bottommargin="0" marginwidth="0" marginheight="0">
<div  style="text-align: left;  margin-left: 100px; width : 600px; margin-bottom:100px">
<h1>Нейросети. Метод обратного распространения</h1>
Недавно в какой-то статье про нейросети на хабре прочитал коммент, примерно такой: "мне кажется, что каждый, кто начинает изучать нейросети считает своим долгом написать статью на хабр". Встречайте еще один такой случай. Я человек, который за все свою жизнь, не запрограммировал ни одной нейросети, зато я разобрался как работает один важный алгоритм в этой науке. Алгоритм обучения сети "метод обратного распространения". Я не смог найти на хабре ни одной статьи, которая объяснила бы вывод и суть алгоритма. Приглашаю в небольшое путешествие в нейросети с небольшим математическим уклоном.<br>
В современном мире при помощи нейросетей решается огромное количество задач. В этой статье мы будем рассматривать классификационные задачи.
Например по портрету человека нужно определить пол. Или, скажем, по рукописной картинке с цифрой нужно определить, что за цифра.<br>
Исторически компьютерные эксперименты с нейросетями были вдохновлены реальными биологическими нейросетями. Единичным элементом такой модели является так называемый персептрон:
<br>
<br>
КАРТИНКА С ПЕРСЕПТРОНОМ
<br>
<br>
на вход такому перcептрону можно подавать сигналы $x^{in}_1, x^{in}_2,...$ (просто числа) и на выходе получается выходной сигнал $x^{out}$ (тоже просто число). По некоторой дальней аналогии с биологиских нейросетями выходной сигнал считается так:<br>
Все входные сигналы помноженные на некоторые веса складываются вместе:
$x^{in}_1w_1 + x^{in}_2w_2 + x^{in}_3w_3$, и если получившееся число выше некоторого порога $b$, то выходной сигнал $x_{out}$ принимается равным одному, если меньше $b$, то нулю. Получается, что
$$x^{out} = \sigma_b(x^{in}_1w_1 + x^{in}_2w_2 + x^{in}_3w_3)$$, где $\sigma_b - ступенчатая функция$:
<br>
<br>
ГРАФИК СТУПЕНЧАТОЙ ФУНКЦИИ
<br>
<br>
На самом деле на практике такие резко ступенчатые функции не применяют. Одна из причин в том, что с такими функциями очень тяжело работать.
Они не гладкие - их невозможно нормально дифференцировать. Поэтому вместо строгой ступенчатой функции применяют более гладкие. Например можно такую $\sigma_b(x) = frac{1}{1-e^{b-x}}$:
<br>
<br>
ГРАФИK СИГМОИДНОЙ ФУНКЦИИ
<br>
<br>
Можно брать и другие функции по желанию.<br>
Мы получаем, что каждому перспетрону сооветствует самая обычная фукнция:
$$
  \sigma_b(x^{in}_1w_1 + x^{in}_2w_2 + x^{in}_3w_3 + ...)
$$
В этой функция полностью определяется весами $w_1, w_2, w3,...$ и еще параметром $b$<br>













 </body>
