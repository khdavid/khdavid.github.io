<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head>
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>David Khudaverdyan.</title><base target="_top">
 
<script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
<script type="text/javascript" src="../menu.js"></script>
<script type="text/javascript" src="../spoiler.js"></script>
</head>
<link rel="stylesheet" type="text/css" href="../main.css">
<body  text="#000000" bgcolor="#FFFFFF" topmargin="0" leftmargin="0" rightmargin="0" bottommargin="0" marginwidth="0" marginheight="0">
<div  style="text-align: left;  margin-left: 100px; width : 600px; margin-bottom:100px">
<h1>Нейросети. Метод обратного распространения</h1>
Недавно в какой-то статье про нейросети на хабре прочитал коммент, примерно такой: "мне кажется, что каждый, кто начинает изучать нейросети считает своим долгом написать статью на хабр". Встречайте еще один такой случай. Я человек, который за все свою жизнь, не запрограммировал ни одной нейросети, зато я разобрался как работает один важный алгоритм в этой науке. Алгоритм обучения сети "метод обратного распространения". Я не смог найти на хабре ни одной статьи, которая объяснила бы вывод и суть алгоритма. Приглашаю в небольшое путешествие в нейросети с небольшим математическим уклоном.<br>
В современном мире при помощи нейросетей решается огромное количество задач. В этой статье мы будем рассматривать классификационные задачи.
Например по портрету человека нужно определить пол. Или, скажем, по рукописной картинке с цифрой нужно определить, что за цифра.<br>
Исторически компьютерные эксперименты с нейросетями были вдохновлены реальными биологическими нейросетями. Единичным элементом такой модели является так называемый персептрон:
<br>
<br>
КАРТИНКА С ПЕРСЕПТРОНОМ
<br>
<br>
на вход такому перcептрону можно подавать сигналы $x^{in}_1, x^{in}_2,...$ (просто числа) и на выходе получается выходной сигнал $x^{out}$ (тоже просто число). По некоторой дальней аналогии с биологиских нейросетями выходной сигнал считается так:<br>
Все входные сигналы помноженные на некоторые веса складываются вместе:
$x^{in}_1w_1 + x^{in}_2w_2 + x^{in}_3w_3$, и если получившееся число выше некоторого порога $b$, то выходной сигнал $x_{out}$ принимается равным одному, если меньше $b$, то нулю. Получается, что
$$x^{out} = \sigma_b(x^{in}_1w_1 + x^{in}_2w_2 + x^{in}_3w_3)$$, где $\sigma_b - ступенчатая функция$:
<br>
<br>
ГРАФИК СТУПЕНЧАТОЙ ФУНКЦИИ
<br>
<br>
На самом деле на практике такие резко ступенчатые функции не применяют. Одна из причин в том, что с такими функциями очень тяжело работать.
Они не гладкие - их невозможно нормально дифференцировать. Поэтому вместо строгой ступенчатой функции применяют более гладкие. Например можно такую $\sigma_b(x) = frac{1}{1-e^{b-x}}$:
<br>
<br>
ГРАФИK СИГМОИДНОЙ ФУНКЦИИ
<br>
<br>
Можно брать и другие функции по желанию.<br>
После того как мы выбрали и раз и навсегда зафиксировали ступенчатую функцию $\sigma_b(x)$ мы получаем, что каждому перспетрону строго на-строго сооветствует самая обычная фукнция:
$$
  \sigma_b(x^{in}_1w_1 + x^{in}_2w_2 + x^{in}_3w_3 + ...)
$$
В этой функция полностью определяется весами $w_1, w_2, w3,...$ и параметром активаыии $b$. Меняя веса или параметр активации можно "настраивать" персептрон.<br>
Из таких персептронов можно насобирать совершенно разные конфигурации нейросетей. Например вот такую слоистую:
<br>
<br>
КАРТИНКА НЕЙРОСЕТИ
<br>
<br>
Эта нейросеть состоит из соединенных перспетронов. Каждый персептрон это отдельная "личность" со своими уникальными наборами параметров. И как нам подсказывает интуиция, основываясь на боилогической аналогии, такая нейросеть может решать какие-то простые для живого, но сложно алгоритмизируемые задачи.
Как это делают. Например нам нужно сделать нейросеть, которая отвечает есть ли на картинке кошка или нет. Для этого нам понадобятся очень много картинок с котиками и без котиков. Мы все картинки сожмем до одинакового разрешения, скажем сто на сто. Получается 10000 пикселей на картинке. Давайте для простоты сделаем картинки черно-белыми. Получается каждый пиксель задается просто напросто числом (0 - совсем черный, 1 - белый, а промежуточные значения это степени серости). Получается, что каждая наша картинка это просто набор из десяти тысяч чисел, просто вектор, например такой:
$$(0.00, 0.00, 1.00, 0.34, 0.23, ....., 0.91)$$
Возьмем нейросети нападобие (рис с нейросетью) у которого 10 тысяч входов и 1 выход и скормим эту картинку (вектор из чисел) этой нейросети.
Нейросеть просто превратит ее в число от нуля до одного. Получаем, что наша нейросеть это самая обычная функция, которая довольно хитрым образом превращает десять тысяч чисел в какое-то одно число. 
$$f(x_1, x_2, x_3,...., x_10000)$$
Эта функция, конечно-же можно "настраивать" изменяя по-отдельности веса в перспетронах.
Теперь давайте разберемся как мы будем решать нашу изначальную задачку с котиками. Итак у нас есть очень много картинок с котиками и без, считай, много векторов с десятью тысячьями чиселок в каждой. Каждый такой набор чисел давайте "скормим сети", то есть посчитаем значения функции f на всех векторав. Договоримся, что чем ближе полученное число к нулю, тем с большей вероятностью на соответствующей картинке котик.
Совершенно очевидно, что изначально рандомно заданная сеть с рандомными весами совершенно чихоать хотела на котиков и будет выдавать результаты совершенно не имеющие отношения к жизни














 </body>
