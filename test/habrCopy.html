Upd: Онлайн сервис, генерирующий математические формулы для этой статьи решил стать капризным, поэтому, если у вас есть проблемы с отображением формул, пока читайте копию статьи из <a href="https://web.archive.org/web/20150616221148/http://habrahabr.ru/post/166693/">вебархива</a>. В скором времени я перенесу формулы на другой сервис.
<img src="http://habrastorage.org/storage2/445/357/969/4453579698b1cecdc820557b719a4f54.png"/>

В интернете, в том числе и на хабре, можно найти много информации про фильтр Калмана. Но тяжело найти легкоперевариваемый вывод самих формул. Без вывода вся эта наука воспринимается как некое шаманство, формулы выглядят как безликий набор символов, а главное, многие простые утверждения, лежащие на поверхности теории, оказываются за пределами понимания. Целью этой статьи будет рассказать об этом фильтре на как можно более доступном языке.
Фильтр Калмана - это мощнейший инструмент фильтрации данных. Основной его принцип состоит в том, что при фильтрации используется информация о физике самого явления. Скажем, если вы фильтруете данные со спидометра машины, то инерционность машины дает вам право воспринимать слишком быстрые скачки скорости как ошибку измерения. Фильтр Калмана интересен тем, что в каком-то смысле, это самый лучший фильтр. Подробнее обсудим ниже, что конкретно означают слова "самый лучший".  В конце статьи я покажу, что во многих случаях формулы можно до такой степени упростить, что от них почти ничего и не останется. 
<habracut />
<h1>Ликбез</h1>
Перед знакомством с фильтром Калмана я предлагаю вспомнить некоторые простые определения и факты из теории вероятности. 

<h4> Случайная величина</h4>
Когда говорят, что дана случайная величина <img src="https://habrastorage.org/files/38e/951/31e/38e95131ebb2473681f7332164b6eb2d.png"/>, то имеют ввиду, что эта величина может принимать случайные значения. Разные значения она принимает с разной вероятностью.  Когда вы кидаете, скажем, кость, то выпадет дискретное множество значений: <img src="https://latex.codecogs.com/png.latex?\{1,2,3,4,5,6\}" title="\{1,2,3,4,5,6\}" />. Когда речь идет, например, о скорости блуждающей частички, то, очевидно, приходится иметь дело с непрерывным множеством  значений. "Выпавшие" значения случайной величины <img src="https://latex.codecogs.com/png.latex?\xi" title="\xi" /> мы будем обозначать  через <img src="https://latex.codecogs.com/png.latex?x_1,x_2..," title="x_1,x_2,..." /> но иногда, будем использовать ту же букву, которой обозначаем случайную величину:  <img src="https://latex.codecogs.com/png.latex?\xi_1, \xi_2,..."  />  
В случае с непрерывным множеством значений случайную величину характеризует плотность вероятности <img src="https://latex.codecogs.com/png.latex?\rho(x)" title="\rho(x)" />, которая нам диктует, что вероятность того, что случайная величина "выпадет" в маленькой окрестности точки <img src="https://latex.codecogs.com/png.latex?x" title="x" /> длиной <img src="https://latex.codecogs.com/png.latex?dx" title="dx" />  равна <img src="https://latex.codecogs.com/png.latex?\rho(x)dx" title="\rho(x)dx" />. Как мы видим из картинки, эта вероятность равна  площади заштрихованного прямоугольника под графиком:
<img src="http://habrastorage.org/storage2/ebd/c38/dbe/ebdc38dbe927cc6c0025a6479f8ec788.png"/> 

Довольно часто в жизни случайные величины распределены по  Гауссу, когда плотность вероятности равна <img src="https://latex.codecogs.com/png.latex?\rho(x)\sim e^{ -\frac{(x-\mu)^2}{2\sigma^2}}" title="\varphi(x)\sim e^{ -\frac{(x-\mu)^2}{2\sigma^2}}" />.
<img src="http://habrastorage.org/storage2/08c/916/ab6/08c916ab6f5a376224903717302fbbce.png"/>
Мы видим, что функция <img src="https://latex.codecogs.com/png.latex?\rho(x)" title="\rho(x)" /> имеет форму колокола с центром в точке <img src="https://latex.codecogs.com/png.latex?\mu" title="\mu" /> и с характерной шириной порядка <img src="https://latex.codecogs.com/png.latex?\sigma" title="\sigma" />.
Раз мы заговорили о Гауссовом распределении, то грешно будет не упомянуть, откуда оно возникло. Также как и числа <img src="https://latex.codecogs.com/png.latex?e" title="e" /> и <img src="https://latex.codecogs.com/png.latex?\pi" title="\pi" /> прочно обосновались в математике и встречаются в самых неожиданных местах, так и распределение Гаусса пустило глубокие корни в теорию вероятности. Одно замечательное утверждение, частично объясняющее Гауссово всеприсутствие, состоит в следующем: 
Пусть есть случайная величина <img src="https://latex.codecogs.com/png.latex?\xi," title="\xi" /> имеющая произвольное распределение (на самом деле существуют некие ограничения на эту произвольность, но они совершенно не жесткие). Проведем <img src="https://latex.codecogs.com/png.latex?n" title="n" /> экспериментов и посчитаем сумму <img src="https://latex.codecogs.com/png.latex?\xi_1+...+\xi_n" title="\xi_1+...+\xi_n" /> "выпавших" значений случайной величины. Сделаем  много таких экспериментов. Понятно, что каждый раз мы будем получать разное значение суммы. Иными словами, эта сумма является сама по себе случайной величиной со своим каким-то определенным законом распределения. Оказывается, что при достаточно больших <img src="https://latex.codecogs.com/png.latex?n" title="n" /> закон распределения этой  суммы стремится к распределению  Гаусса (к слову, характерная ширина "колокола" растет как <img src="https://latex.codecogs.com/png.latex?\sqrt{n}" title="\sqrt{n}" />).  Более подробно читаем в википедии: <a href="http://ru.wikipedia.org/wiki/%D0%A6%D0%B5%D0%BD%D1%82%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0">центральная предельная теорема</a>. В жизни очень часто встречаются величины, которые складываются из большого количества одинаково распределенных независимых случайных величин, поэтому и распределены по Гауссу.

<h4> Среднее значение </h4>
Среднее значение случайной величины - это то, что  мы получим в пределе, если проведем очень много экспериментов, и посчитаем среднее арифметическое выпавших значений. Среднее значение обозначают по-разному: математики любят обозначать через <img src="https://latex.codecogs.com/png.latex?M\xi" title="M\xi" /> (математическое ожидание), а заграничные математики через  <img src="https://latex.codecogs.com/png.latex?E\xi" title="E\xi" /> (expectation). Физики же через <img src="https://latex.codecogs.com/png.latex?\overline{\xi}" title="\overline{\xi}" /> или <img src="https://latex.codecogs.com/png.latex?\textless\xi\textgreater"  />. Мы будем обозначать на заграничный лад:  <img src="https://latex.codecogs.com/png.latex?E\xi" title="E\xi" />. 
Например, для Гауссова распределения <img src="https://latex.codecogs.com/png.latex?\rho(x)\sim e^{ -\frac{(x-\mu)^2}{2\sigma^2}}" title="\varphi(x)\sim e^{ -\frac{(x-\mu)^2}{2\sigma^2}}" />, среднее значение равно <img src="https://latex.codecogs.com/png.latex?\mu" title="\mu" />.

<h4> Дисперсия </h4>
В случае с  распределением Гаусса мы совершенно четко видим, что случайная величина предпочитает выпадать в некоторой окрестности своего среднего значения <img src="https://latex.codecogs.com/png.latex?\mu" title="\mu" />.

<spoiler title="Еще раз полюбоваться  распределением Гаусса">
<img src="http://habrastorage.org/storage2/08c/916/ab6/08c916ab6f5a376224903717302fbbce.png"/>
</spoiler>
Как видно из графика, характерный разброс значений порядка <img src="https://latex.codecogs.com/png.latex?\sigma" title="\sigma" />. Как же оценить этот разброс значений для произвольной случайной величины, если мы знаем ее распределение.  Можно нарисовать график ее плотности вероятности и оценить характерную ширину на глаз. Но мы предпочитаем идти алгебраическим путем. Можно найти среднюю длину (модуль) отклонения  от среднего значения:  <img src="https://latex.codecogs.com/png.latex?E|\xi-E\xi|" title="E|\xi-E\xi|" />. Эта величина будет хорошей оценкой характерного разброса значений <img src="https://latex.codecogs.com/png.latex?\xi" title="\xi" />. Но мы с вами очень хорошо знаем, что использовать модули в формулах - одна головная боль, поэтому эту формулу редко используют для оценок характерного разброса.
Более  простой способ (простой в смысле расчетов) - найти <img src="https://latex.codecogs.com/png.latex?E(\xi-E\xi)^2" />. Эту величину называют дисперсией, и часто обозначают как <img src="https://latex.codecogs.com/png.latex?\sigma_\xi^2" title="\sigma_\xi^2" />.  Корень из  дисперсии - хорошая оценка разброса случайной величины. Корень из дисперсии еще называют среднеквадратичным отклонением.
Например, для распределение Гаусса <img src="https://latex.codecogs.com/png.latex?\rho(x)\sim e^{ -\frac{(x-\mu)^2}{2\sigma^2}}," title="\varphi(x)\sim e^{ -\frac{(x-\mu)^2}{2\sigma^2}}" /> можно посчитать, что определенная выше дисперсия <img src="https://latex.codecogs.com/png.latex?\sigma_\xi^2" title="\sigma_\xi^2" /> в точности равна  <img src="https://latex.codecogs.com/png.latex?\sigma^2" title="\sigma^2" />, а значит среднеквадратичное отклонение равно <img src="https://latex.codecogs.com/png.latex?\sigma" title="\sigma" />, что очень хорошо согласуется с нашей геометрической интуицией. 
На самом деле тут скрыто маленькое мошенничество. Дело в том, что в определении распределения Гаусса под экспонентой стоит выражение <img src="https://latex.codecogs.com/png.latex?\dpi{70}-\frac{(x-\mu)^2}{2\sigma^2}" title="-\frac{(x-\mu)^2}{2\sigma^2}" />. Эта двойка в знаменателе стоит именно для того, чтобы среднеквадратичное отклонение <img src="https://latex.codecogs.com/png.latex?\sigma_\xi" title="\sigma_\xi" /> равнялось бы коэффициенту <img src="https://latex.codecogs.com/png.latex?\sigma" title="\sigma" />. То есть сама формула распределения Гаусса написана в виде, специально заточенном для того, что мы будем считать ее среднеквадратичное отклонение.

<h4>Независимые случайные величины</h4>
Случайные величины бывают зависимыми и нет. Представьте, что вы бросаете иголку на плоскость и записываете координаты ее обоих концов. Эти две координаты зависимы, они связаны условием, что расстояние между ними всегда равно длине иголки, хотя и являются случайными величинами. 
Случайные величины независимы, если результат выпадения первой из них совершенно не зависит от результата выпадения второй из них. Если случайные величины <img src="https://latex.codecogs.com/png.latex?\xi_1" title="\xi_1" /> и <img src="https://latex.codecogs.com/png.latex?\xi_2" title="\xi_2" /> независимы, то среднее значение их произведения равно произведению их средних значений:

<img src="https://latex.codecogs.com/png.latex?E(\xi_1\cdot\xi_2)=E\xi_1\cdot E\xi_2" title="E(\xi_1\cdot\xi_2)=E\xi_1\cdot E\xi_2" />

<spoiler title="Доказательство">
Например, иметь голубые глаза и окончить школу с золотой медалью - независимые случайные величины. Если голубоглазых, скажем <img src="https://latex.codecogs.com/png.latex?20\%=0.2," title="20\%=0.2" /> а золотых медалистов <img src="https://latex.codecogs.com/png.latex?5\%=0.05" title="5\%=0.05" />, то голубоглазых медалистов <img src="https://latex.codecogs.com/png.latex?0.2\cdot 0.05=0.01=1\%."  /> Этот пример подсказывает нам, что если случайные величины <img src="https://latex.codecogs.com/png.latex?\xi_1" title="\xi_1" /> и <img src="https://latex.codecogs.com/png.latex?\xi_1" title="\xi_1" /> заданы своими плотностями вероятности <img src="https://latex.codecogs.com/png.latex?\rho_1(x)" title="\rho_1(x)" /> и <img src="https://latex.codecogs.com/png.latex?\rho_2(y)" title="\rho_2(y)" />, то независимость этих величин выражается в том, что плотность вероятности <img src="https://latex.codecogs.com/png.latex?\rho(x,y)" title="\rho(x,y)" /> (первая величина выпала <img src="https://latex.codecogs.com/png.latex?x" title="x" />, а вторая <img src="https://latex.codecogs.com/png.latex?y" title="y" />) находится по формуле:
<img src="https://latex.codecogs.com/png.latex?\rho(x,y)=\rho_1(x)\cdot\rho_2(y)" title="\rho(x,y)=\rho_1(x)\cdot\rho_2(y)" />
Из этого сразу же следует, что:
<img src="https://latex.codecogs.com/png.latex?\begin{array}{l} \displaystyle E(\xi_1\cdot\xi_2)=\int xy\rho(x,y)dxdy=\int xy\rho_1(x)\rho_2(y)dxdy=\\ \displaystyle \int x\rho_1(x)dx\int y\rho_2(y)dy=E\xi_1\cdot E\xi_2 \end{array}" title="\begin{array}{l} \displaystyle E(\xi_1\cdot\xi_2)=\int xy\rho(x,y)dxdy=\int xy\rho_1(x)\rho_2(y)dxdy=\\ \displaystyle \int x\rho_1(x)dx\int y\rho_2(y)dy=E\xi_1\cdot E\xi_2 \end{array}" />
Как вы видите,  доказательство проведено  для случайных величин, которые имеют непрерывный спектр значений и заданы своей плотностью вероятности. В других случаях идея доказательтсва аналогичная.
</spoiler>
<h1>Фильтр Калмана</h1>
<h4>Постановка задачи</h4>
Обозначим за <img src="https://latex.codecogs.com/png.latex?x_k" title="x_k" />  величину, которую мы будем измерять, а потом фильтровать.  Это может быть координата, скорость, ускорение, влажность, степень вони, температура, давление, и т.д.
Начнем с простого примера, который и приведет нас к формулировке общей задачи. Представьте себе, что у нас есть радиоуправляемая машинка, которая может ехать только вперед и назад. Мы, зная вес машины, форму, покрытие дороги и т.д., расcчитали как контролирующий джойстик влияет на скорость движения <img src="https://latex.codecogs.com/png.latex?v_k"/>. 

<img src="http://habrastorage.org/storage2/f3b/ac3/09c/f3bac309c4975ab560e4fd36a75349e9.png"/>

Тогда координата машины будет изменяться по закону:

<img src="https://latex.codecogs.com/png.latex?x_{k+1}=x_k+v_kdt" />

В реальной же жизни мы не можем учесть в наших расчетах маленькие возмущения, действующие на машину (ветер, ухабы, камушки на дороге), поэтому настоящая скорость машины будет отличаться от расчетной. К правой части написанного уравнения добавится случайная величина <img src="https://latex.codecogs.com/png.latex?\xi_k" />: 

<img src="https://latex.codecogs.com/png.latex?x_{k+1}=x_k+v_k dt+\xi_k" />  

У нас есть установленный на машинке GPS сенсор, который пытается мерить истинную координату <img src="https://latex.codecogs.com/png.latex?x_k" title="x_k" /> машинки, и ,конечно же, не может ее померить точно, а мерит с ошибкой <img src="https://latex.codecogs.com/png.latex?\eta_k"  />, которая является тоже случайной величиной. В итоге с сенсора мы получаем ошибочные данные:

<img src="https://latex.codecogs.com/png.latex?z_k=x_k+\eta_k" title="z_k=x_k+\eta_k" />

Задача состоит в том, что, зная  неверные показания сенсора <img src="https://latex.codecogs.com/png.latex?z_k" title="z_k" />, найти хорошее приближение  для истинной координаты машины   <img src="https://latex.codecogs.com/png.latex?x_k" title="x_k" />. Это хорошее приближение мы будем обозначать как <img src="https://latex.codecogs.com/png.latex?x^{opt}_k" title="x^{opt}_k" />.
 В формулировке же общей задачи, за координату <img src="https://latex.codecogs.com/png.latex?x_k"  /> может отвечать все что угодно (температура, влажность...), а  член, отвечающий за контроль системы извне мы обозначим за <img src="https://latex.codecogs.com/png.latex?u_k" /> (в примере c машиной <img src="https://latex.codecogs.com/png.latex?u_k=v_k\cdot dt" />). Уравнения для координаты и показания сенсора будут выглядеть так:

<img src="http://habrastorage.org/storage2/6b9/e56/29f/6b9e5629f1d2bb9c868b600483d8f8e9.png"/>&nbsp;&nbsp;&nbsp;&nbsp;(1)

Давайте подробно обсудим, что нам известно:

<ul>
<li> <img src="https://latex.codecogs.com/png.latex?u_k" title="u_k" /> - это известная величина, которая контролирует эволюцию системы. Мы ее знаем из построенной нами физической модели. 
</li>
<li>
Ошибка модели <img src="https://latex.codecogs.com/png.latex?\xi_k" /> и ошибка сенсора <img src="https://latex.codecogs.com/png.latex?\eta_k" />- случайные величины.  И их законы распределения не зависят от времени (от номера итерации <img src="https://latex.codecogs.com/png.latex?k" title="k" />).
</li>
<li>
Средние  значения ошибок равны нулю: <img src="https://latex.codecogs.com/png.latex?E\xi_k=E\eta_k=0" />. 
</li>
<li>
Сам закон распределения случайных величин может быть нам и не известен, но известны их дисперсии <img src="https://latex.codecogs.com/png.latex?\sigma_\xi^2" title="\sigma_\xi^2" /> и <img src="https://latex.codecogs.com/png.latex?\sigma_\eta^2" title="\sigma_\eta^2" />. Заметим, что дисперсии не зависят от <img src="https://latex.codecogs.com/png.latex?k"  />, потому  что законы распределения не зависят от него.
</li>
<li>
Предполагается, что все случайные ошибки независимы друг от друга: какая ошибка будет в момент времени <img src="https://latex.codecogs.com/png.latex?k" title="k" /> совершенно не зависит от ошибки в другой момент времени <img src="https://latex.codecogs.com/png.latex?k'" title="k'" />.
</li>
</ul>
Нелишним будет отметить, что задача фильтрации - это не задача сглаживания. Мы не стремимся сглаживать данные с сенсора,  мы стремимся получить наиболее близкое значение к реальной координате <img src="https://latex.codecogs.com/png.latex?x_k" title="x_k" />. 

<h4>Алгоритм Калмана </h4>
Мы будем рассуждать по индукции. Представьте себе, что на <img src="https://latex.codecogs.com/png.latex?k" />-ом шаге мы уже нашли отфильтрованное значение с сенсора <img src="https://latex.codecogs.com/png.latex?x^{opt}_k" title="x^{opt}_k" />, которое хорошо приближает истинную координату системы <img src="https://latex.codecogs.com/png.latex?x_k" title="x_k" />. Не забываем, что мы знаем уравнение, контролирующее изменение нам неизвестной координаты: 

<img src="https://latex.codecogs.com/png.latex?x_{k+1}=x_k+u_k+\xi_k" title="x_{k+1}=x_k+u_k+\xi_k" /> 

поэтому, еще не получая значение с сенсора, мы можем предположить, что на шаге <img src="https://latex.codecogs.com/png.latex?k+1" title="k+1" /> система эволюционирует согласно этому закону и сенсор  покажет что-то близкое к <img src="https://latex.codecogs.com/png.latex?x^{opt}_{k}+u_k" title="x^{opt}_{k}+u_k" />. К сожалению, пока мы не можем сказать ничего более точного. С другой стороны, на шаге <img src="https://latex.codecogs.com/png.latex?k+1" title="k+1" />  у нас на руках будет неточное показание сенсора <img src="https://latex.codecogs.com/png.latex?z_{k+1}" title="z_{k+1}" />.
 Идея Калмана состоит в том, что чтобы получить наилучшее приближение к истинной координате <img src="https://latex.codecogs.com/png.latex?x_{k+1}" title="x_{k+1}" />, мы должны выбрать золотую середину между показанием <img src="https://latex.codecogs.com/png.latex?z_{k+1}"  /> неточного сенсора   и <img src="https://latex.codecogs.com/png.latex?x^{opt}_{k}+u_k" title="x^{opt}_{k}+u_k" /> - нашим предсказанием   того, что мы ожидали от него увидеть. Показанию сенсора мы дадим вес <img src="https://latex.codecogs.com/png.latex?K," title="K" /> а на предсказанное значение останется вес <img src="https://latex.codecogs.com/png.latex?(1-K)" />:

<img src="https://latex.codecogs.com/png.latex?x^{opt}_{k+1}=K\cdot z_{k+1}+(1-K)\cdot(x^{opt}_k+u_k)" />

Коэффициент <img src="https://latex.codecogs.com/png.latex?K"> называют коэффициентом Калмана. Он зависит от шага итерации, поэтому правильнее было бы писать <img src="https://latex.codecogs.com/png.latex?K_{k+1}" title="K_{k+1}" />, но пока, чтобы не загромождать формулы расчетах, мы будем опускать его индекс. 
Мы должны выбрать коэффициент Калмана <img src="https://latex.codecogs.com/png.latex?K" title="K" /> таким, чтобы получившееся оптимальное значение координаты <img src="https://latex.codecogs.com/png.latex?x^{opt}_{k+1}" title="x^{opt}_{k+1}" /> было бы наиболее близко к истинной  координате <img src="https://latex.codecogs.com/png.latex?x_{k+1}" title="x_{k+1}" />. К примеру, если мы знаем, что наш сенсор очень точный, то мы будем больше доверять его показанию и дадим значению <img src="https://latex.codecogs.com/png.latex?z_{k+1}" title="z_{k+1}" /> больше весу (<img src="https://latex.codecogs.com/png.latex?K" title="K" /> близко единице). Eсли же сенсор, наоборот, совсем не точный, тогда больше будем ориентироваться на теоретически предсказанное значение <img src="https://latex.codecogs.com/png.latex?x^{opt}_{k}+u_k" title="x^{opt}_{k}+u_k" />.
В общем случае, чтобы найти точное значение коэффициента Калмана <img src="https://latex.codecogs.com/png.latex?K" title="K" />, нужно просто минимизировать ошибку:

<img src="https://latex.codecogs.com/png.latex?e_{k+1}=x_{k+1}-x^{opt}_{k+1}" title="e_{k+1}=x_{k+1}-x^{opt}_{k+1}" />

Используем уравнения (1)  (те которые на голубом фоне в рамочке), чтобы переписать выражение для ошибки:

<img src="https://latex.codecogs.com/png.latex?\begin{array}{l} e_{k+1}=(1-K)(e_k+\xi_k)-K\eta_{k+1} \end{array}" />

<spoiler title="Доказательство">
<img src="https://latex.codecogs.com/png.latex?\begin{array}{l} e_{k+1}=x_{k+1}-x^{opt}_{k+1}=x_{k+1}-Kz_{k+1}-(1-K)(x^{opt}_k+u_k)=\\ =x_k+u_k+\xi_k-K(x_k+u_k+\xi_k+\eta_{k+1})-(1-K)(x^{opt}_k+u_k)=\\ =(1-K)(x_k-x_k^{opt}+\xi_k)-K\eta_{k+1}=(1-K)(e_k+\xi_k)-K\eta_{k+1} \end{array}" title="\begin{array}{l} e_{k+1}=x_{k+1}-x^{opt}_{k+1}=x_{k+1}-Kz_{k+1}-(1-K)(x^{opt}_k+u_k)=\\ =x_k+u_k+\xi_k-K(x_k+u_k+\xi_k+\eta_{k+1})-(1-K)(x^{opt}_k+u_k)=\\ =(1-K)(x_k-x_k^{opt}+\xi_k)-K\eta_{k+1}=(1-K)(e_k+\xi_k)-K\eta_{k+1} \end{array}" />
</spoiler>
Теперь самое время обсудить, что означает выражение минимизировать ошибку? Ведь ошибка, как мы видим, сама по себе является случайной величиной и каждый раз принимает разные значения. На самом деле не существует однозначного подхода к определению того, что означает, что ошибка минимальна. Точно как и в случае с дисперсией случайной величины, когда мы пытались оценить характерную ширину ее разброса, так и тут мы выберем самый простой для расчетов критерий. Мы будем минимизировать среднее значение от квадрата ошибки:

<img src="https://latex.codecogs.com/png.latex?E(e^2_{k+1})\rightarrow \mathrm{min}" title="E(e^2_{k+1})\rightarrow \mathrm{min}" />

Распишем последнее выражение:

<img src="https://latex.codecogs.com/png.latex? E(e_{k+1}^2)=(1-K)^2(Ee_k^2+\sigma_\xi^2)+K^2\sigma_\eta^2"  />

<spoiler title="Доказательство">
Из того что все случайные величины, входящие в выражение для <img src="https://latex.codecogs.com/png.latex?e_{k+1}" title="e_{k+1}" />, независимы, следует, что все  "перекрестные" члены равны нулю:
<img src="https://latex.codecogs.com/png.latex?E(\xi_{k}\eta_{k+1})=E(e_k\xi_k)=E(e_k\eta_{k+1})=0" title="E(\xi_{k}\eta_{k+1})=E(e_k\xi_k)=E(e_k\eta_{k+1})=0" />
Еще мы использовали тот факт, что <img src="https://latex.codecogs.com/png.latex?E\eta_{k+1}=E\xi_{k}=0" title="E\eta_{k+1}=E\xi_{k}=0" />, тогда формула для дисперсий выглядит намного проще: <img src="https://latex.codecogs.com/png.latex?\sigma^2_\xi=E\xi_k^2" /> и <img src="https://latex.codecogs.com/png.latex?\sigma^2_\eta=E\eta^2_{k+1}"/>.
</spoiler>

Это выражение принимает минимальное значение, когда (приравниваем производную к нулю)

<img src="https://latex.codecogs.com/png.latex?K_{k+1}=\frac{Ee_k^2+\sigma_\xi^2}{Ee_k^2+\sigma_\xi^2+\sigma_\eta^2}" title="K_{k+1}=\frac{Ee_k^2+\sigma_\xi^2}{Ee_k^2+\sigma_\xi^2+\sigma_\eta^2}" />

Здесь мы уже пишем выражение для коэффициента Калмана с индексом шага <img src="https://latex.codecogs.com/png.latex?k+1" title="k+1" />, тем самым мы подчеркиваем, что он зависит от шага итерации.
Подставляем в выражение для среднеквадратичной ошибки  <img src="https://latex.codecogs.com/png.latex?E(e^2_{k+1})" title="E(e^2_{k+1})" /> минимизирующее ее значение коэффициента Калмана <img src="https://latex.codecogs.com/png.latex?K_{k+1}"/> . Получаем:

<img src="https://latex.codecogs.com/png.latex?E(e^2_{k&plus;1})=\frac{\sigma_\eta^2(Ee_k^2&plus;\sigma_\xi^2)}{Ee_k^2&plus;\sigma_\xi^2&plus;\sigma_\eta^2}" title="E(e^2_{k+1})=\frac{\sigma_\eta^2(Ee_k^2+\sigma_\xi^2)}{Ee_k^2+\sigma_\xi^2+\sigma_\eta^2}" />.

Наша задача решена. Мы получили итерационную формулу, для вычисления коэффициента Калмана. 

<spoiler title="Все формулы в одном месте">
<img src="http://habrastorage.org/storage2/a7b/7a0/1c5/a7b7a01c59d4b2910dd4478206929a46.png"/>
</spoiler>

<h4>Пример</h4>
На рекламной картинке в начале статьи отфильтрованы данные с вымышленного GPS сенсора, установленного на вымышленной машине, которая едет равноускоренно c известным вымышленным ускорением <img src="https://latex.codecogs.com/png.latex?a" title="a" />.

<img src="https://latex.codecogs.com/png.latex?x_{t+1}=x_t+at\cdot dt+\xi_t" title="x_{t+1}=x_t+at\cdot dt+\xi_t" />

<spoiler title="Еще раз посмотреть на результат фильтрования">
<img src="http://habrastorage.org/storage2/445/357/969/4453579698b1cecdc820557b719a4f54.png"/>
</spoiler>
<spoiler title="Код на матлабе">
<source lang="Matlab">
clear all;
N=100  % number of samples
a=0.1 % acceleration
sigmaPsi=1
sigmaEta=50;
k=1:N
x=k
x(1)=0
z(1)=x(1)+normrnd(0,sigmaEta);
for t=1:(N-1)
  x(t+1)=x(t)+a*t+normrnd(0,sigmaPsi); 
   z(t+1)=x(t+1)+normrnd(0,sigmaEta);
end;
%kalman filter
xOpt(1)=z(1);
eOpt(1)=sigmaEta; % eOpt(t) is a square root of the error dispersion (variance). It's not a random variable. 
for t=1:(N-1)
  eOpt(t+1)=sqrt((sigmaEta^2)*(eOpt(t)^2+sigmaPsi^2)/(sigmaEta^2+eOpt(t)^2+sigmaPsi^2))
  K(t+1)=(eOpt(t+1))^2/sigmaEta^2
 xOpt(t+1)=(xOpt(t)+a*t)*(1-K(t+1))+K(t+1)*z(t+1)
end;
plot(k,xOpt,k,z,k,x)
</source>
</spoiler>

<h4>Анализ</h4>
Если проследить, как с шагом итерации <img src="https://latex.codecogs.com/png.latex?k" title="k" /> изменяется коэффициент Калмана <img src="https://latex.codecogs.com/png.latex?K_k" title="K_k" />, то можно показать, что он всегда стабилизируется к определенному значению <img src="https://latex.codecogs.com/png.latex?K_{stab}" title="K_{stab}" />. К примеру, когда среднеквадратичные ошибки сенсора и модели относятся друг к другу как десять к одному, то график коэффициента Калмана в зависимости от шага итерации выглядит так:

<img src="http://habrastorage.org/storage2/86e/872/08e/86e87208ece7ba8d17f59fc428cc11b7.png"/>

В следующем примере мы обсудим как это поможет существенно облегчить нашу жизнь.

<h4>Второй пример</h4>
На практике очень часто бывает, что нам вообще ничего не известно о физической модели того, что мы фильтруем. К примеру, вы захотели отфильтровать показания с вашего любимого акселерометра. Вам же заранее неизвестно по какому закону вы намереваетесь крутить акселерометр. Максимум информации, которую вы можете выцепить - это дисперсия ошибки сенсора <img src="https://latex.codecogs.com/png.latex?\sigma^2_{\eta}" title="\sigma^2_{\eta}" />. В такой непростой ситуации все незнание модели движения можно загнать в случайную величину <img src="https://latex.codecogs.com/png.latex?\xi_k" title="\xi_k" />:

<img src="https://latex.codecogs.com/png.latex?x_{k+1}=x_k+\xi_k" title="x_{k+1}=x_k+\xi_k" />

Но, откровенно говоря, такая система уже совершенно не удовлетворяет тем условиям, которые мы налагали на случайную величину <img src="https://latex.codecogs.com/png.latex?\xi_k" title="\xi_k" />, ведь теперь туда запрятана вся неизвестная нам физика движения, и поэтому мы не можем говорить, что в разные моменты времени ошибки модели независимы друг от друга и что их средние значения равны нулю. В этом случае, по большому счету, теория фильтра Калмана не применима. Но, мы не будем обращать внимания на этот факт, а, тупо применим все махину формул, подобрав коэффициенты <img src="https://latex.codecogs.com/png.latex?\sigma^2_\xi" title="\sigma^2_\xi" /> и <img src="https://latex.codecogs.com/png.latex?\sigma^2_\eta" title="\sigma^2_\eta" />  на глаз, так чтобы отфильтрованные данные миленько смотрелись.
Но можно пойти по другому, намного более простому пути. Как мы видели выше, коэффициент Калмана <img src="https://latex.codecogs.com/png.latex?K_k" title="K_k" />  с увеличением  номера шага <img src="https://latex.codecogs.com/png.latex?k" title="k" /> всегда стабилизируется к значению <img src="https://latex.codecogs.com/png.latex?K_{stab}" title="K_{stab}" />. Поэтому вместо того, чтобы подбирать коэффициенты <img src="https://latex.codecogs.com/png.latex?\sigma^2_\xi" title="\sigma^2_\xi" /> и <img src="https://latex.codecogs.com/png.latex?\sigma^2_\eta" title="\sigma^2_\eta" /> и находить по сложным формулам коэффициент Калмана  <img src="https://latex.codecogs.com/png.latex?K_k" title="K_k" />, мы можем считать этот коэффициент всегда константой, и подбирать только эту константу. Это допущение почти ничего не испортит. Во-первых, мы уже и так незаконно пользуемся теорией Калмана, а во-вторых коэффициент Калмана быстро стабилизируется к константе. В итоге все очень упростится. Нам вообще никакие формулы из теории Калмана не нужны, нам просто нужно подобрать приемлемое значение <img src="https://latex.codecogs.com/png.latex?K_{stab}" title="K_{stab}" /> и вставить в итерационную формулу:

<img src="https://latex.codecogs.com/png.latex?x^{opt}_{k+1}=K_{stab}\cdot z_{k+1}+(1-K_{stab})\cdot x^{opt}_k" title="x^{opt}_{k+1}=K_{stab}\cdot z_{k+1}+(1-K_{stab})\cdot x^{opt}_k" />

На следующем графике показаны отфильтрованные двумя разными способами данные с вымышленного сенсора. При условии того, что мы ничего не знаем о физике явления. Первый способ - честный, со всеми формулами из теории Калмана. А второй  - упрощенный, без формул. 

<img src="http://habrastorage.org/storage2/92b/0a4/a06/92b0a4a06d5b0c9aafb27201a0c0c425.png"/>

Как мы видим, методы почти ничем не отличаются. Маленькое отличие наблюдается только вначале, когда коэффициент Калмана еще не стабилизировался.

<h4> Обсуждение</h4>
Как мы увидели, основная  идея фильтра Калмана состоит в том, что надо найти коэффициент <img src="https://latex.codecogs.com/png.latex?K" title="K" /> такой, чтобы отфильтрованное значение 

<img src="https://latex.codecogs.com/png.latex?x^{opt}_{k+1}=Kz_{k+1}+(1-K)(x^{opt}_k+u_k)" />

в среднем меньше всего отличалось бы от реального значения координаты <img src="https://latex.codecogs.com/png.latex?x_{k+1}" title="x_{k+1}" />. Мы видим, что отфильтрованное значение <img src="https://latex.codecogs.com/png.latex?x^{opt}_{k+1}" title="x^{opt}_{k+1}" /> есть линейная функция от показания сенсора <img src="https://latex.codecogs.com/png.latex?z_{k+1}" /> и предыдущего отфильтрованного значения <img src="https://latex.codecogs.com/png.latex?x^{opt}_k" title="x^{opt}_k" />. А предыдущее отфильтрованное значение  <img src="https://latex.codecogs.com/png.latex?x^{opt}_k" title="x^{opt}_k" /> является, в свою очередь, линейной функцией от показания сенсора <img src="https://latex.codecogs.com/png.latex?z_k" title="z_k" /> и предпредыдущего отфильтрованного значения <img src="https://latex.codecogs.com/png.latex?x^{opt}_{k-1}" title="x^{opt}_{k-1}" />. И так далее, пока цепь полностью не развернется. То есть отфильтрованное значение зависит от <b>всех</b> предыдущих показаний сенсора линейно:

<img src="https://latex.codecogs.com/png.latex?x^{opt}_{k+1}=\lambda+\lambda_0z_0+...+\lambda_{k+1}z_{k+1}" title="x^{opt}_{k+1}=\lambda+\lambda_0z_0+...+\lambda_{k+1}z_{k+1}" />

Поэтому фильтр Калмана называют линейным фильтром. 
Можно доказать, что из всех линейных фильтров Калмановский фильтр самый лучший. Самый лучший в том смысле, что средний квадрат ошибки фильтра минимален.

<h4>Многомерный случай</h4>
Всю теорию фильтра Калмана можно обобщить на многомерный случай. Формулы там выглядят чуть страшнее, но сама идея их вывода такая же, как и в одномерном случае. В этой прекрасной статье вы можете увидеть их: <a href="http://habrahabr.ru/post/140274/">http://habrahabr.ru/post/140274/</a>.
А в этом замечательном <a href="http://www.youtube.com/watch?v=FkCT_LV9Syk">видео</a> разобран пример, как их использовать.

<h1>Литература</h1>
Оригинальную статью Калмана можно скачать вот тут: <a href="http://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf">http://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf</a>.